"""
ðŸ¤– The Coordinator - LLM-Powered Multi-Agent Orchestrator

This agent uses Groq's fast LLM API to coordinate the hospital workflow,
enabling intelligent agent collaboration and adaptive treatment planning.
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum

from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown

console = Console()


class MessageType(Enum):
    """Types of messages agents can send"""
    REQUEST = "request"
    RESPONSE = "response"
    FEEDBACK = "feedback"
    INSTRUCTION = "instruction"
    ANALYSIS = "analysis"


@dataclass
class AgentMessage:
    """Message passed between agents"""
    sender: str
    receiver: str
    message_type: MessageType
    content: str
    metadata: Dict[str, Any]
    timestamp: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "sender": self.sender,
            "receiver": self.receiver,
            "message_type": self.message_type.value,
            "content": self.content,
            "metadata": self.metadata,
            "timestamp": self.timestamp
        }


@dataclass
class TreatmentStep:
    """A step in the treatment plan"""
    agent_name: str
    action: str
    instructions: str
    priority: int
    dependencies: List[str]


@dataclass
class TreatmentPlan:
    """Complete treatment plan generated by coordinator"""
    model_id: str
    symptom: str
    strategy: str
    steps: List[TreatmentStep]
    estimated_time: int
    confidence: float
    reasoning: str


class GroqLLM:
    """
    Groq API wrapper for fast LLM inference
    Uses llama-3.1-70b-versatile for intelligent coordination
    """
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        """
        Initialize Groq LLM
        
        Args:
            api_key: Groq API key (or set GROQ_API_KEY env var)
            model: Model to use (or set GROQ_MODEL env var, default: gpt-oss-120b)
        """
        # Load from .env file if it exists
        env_file = Path(__file__).parent.parent.parent / ".env"
        if env_file.exists():
            with open(env_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        if key == "GROQ_API_KEY" and not api_key:
                            api_key = value
                        elif key == "GROQ_MODEL" and not model:
                            model = value
        
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        self.model = model or os.getenv("GROQ_MODEL") or "openai/gpt-oss-120b"  # Default to openai/gpt-oss-120b
        
        if not self.api_key:
            console.print("[yellow]âš ï¸ GROQ_API_KEY not set. Using mock mode.[/yellow]")
            self.mock_mode = True
        else:
            self.mock_mode = False
            try:
                from groq import Groq
                self.client = Groq(api_key=self.api_key)
                console.print(f"[green]âœ… Groq LLM initialized: {self.model}[/green]")
            except ImportError:
                console.print("[yellow]âš ï¸ Groq package not installed. Using mock mode.[/yellow]")
                console.print("[dim]Install with: pip install groq[/dim]")
                self.mock_mode = True
    
    def generate(self, prompt: str, system_prompt: Optional[str] = None, 
                 temperature: float = 0.7, max_tokens: int = 2000) -> str:
        """
        Generate response using Groq
        
        Args:
            prompt: User prompt
            system_prompt: Optional system prompt
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text
        """
        if self.mock_mode:
            return self._mock_generate(prompt)
        
        try:
            messages = []
            
            if system_prompt:
                messages.append({
                    "role": "system",
                    "content": system_prompt
                })
            
            messages.append({
                "role": "user",
                "content": prompt
            })
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=1,
                stream=False
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            console.print(f"[red]âŒ Groq API error: {e}[/red]")
            return self._mock_generate(prompt)
    
    def _mock_generate(self, prompt: str) -> str:
        """Mock generation for testing without API key"""
        if "treatment plan" in prompt.lower():
            return """
            Based on the diagnosis, I recommend:
            
            1. Run comprehensive diagnosis with all safety tests
            2. Generate aggressive cure dataset (200+ examples)
            3. Check skill preservation carefully
            4. Adjust hyperparameters based on severity
            
            Strategy: Aggressive treatment with skill protection
            Confidence: 85%
            """
        elif "analyze" in prompt.lower():
            return "The result looks good. Quality is acceptable. Proceed to next step."
        elif "feedback" in prompt.lower():
            return "Consider adding more diverse examples to improve robustness."
        else:
            return "Understood. Proceeding with the plan."


class CoordinatorAgent:
    """
    LLM-powered coordinator that orchestrates the hospital workflow
    
    Key capabilities:
    - Adaptive treatment planning using Groq LLM
    - Agent-to-agent communication routing
    - Quality analysis and feedback generation
    - Iterative refinement based on results
    """
    
    def __init__(self, groq_api_key: Optional[str] = None):
        """
        Initialize the Coordinator
        
        Args:
            groq_api_key: Groq API key (optional, can use env var)
        """
        self.llm = GroqLLM(api_key=groq_api_key)
        self.conversation_history: List[AgentMessage] = []
        self.current_plan: Optional[TreatmentPlan] = None
        
        # Agent registry (will be populated when agents are added)
        self.agents: Dict[str, Any] = {}
    
    def register_agent(self, name: str, agent: Any):
        """Register an agent with the coordinator"""
        self.agents[name] = agent
        console.print(f"[green]âœ… Registered agent: {name}[/green]")
    
    def plan_treatment(self, model_id: str, symptom: str, 
                      diagnosis_preview: Optional[Dict] = None) -> TreatmentPlan:
        """
        Use Groq LLM to create adaptive treatment plan
        
        Args:
            model_id: Model to treat
            symptom: Reported symptom
            diagnosis_preview: Optional preliminary diagnosis data
            
        Returns:
            TreatmentPlan with intelligent strategy
        """
        console.print(Panel.fit(
            f"ðŸ¤– [bold blue]Coordinator Planning Treatment[/bold blue]\n"
            f"Patient: {model_id}\n"
            f"Symptom: {symptom}",
            border_style="blue"
        ))
        
        # Build context for LLM
        context = f"""
Patient Model: {model_id}
Reported Symptom: {symptom}
"""
        
        if diagnosis_preview:
            context += f"\nPreliminary Diagnosis:\n{json.dumps(diagnosis_preview, indent=2)}"
        
        # Generate treatment plan using Groq
        system_prompt = """You are a medical coordinator for AI model repair at Oumi Hospital.

You coordinate 4 specialist agents:
1. Diagnostician - Runs comprehensive tests (safety, hallucination, bias)
2. Pharmacist - Creates cure datasets using synthesis
3. Neurologist - Checks skill preservation (math, reasoning, writing, factual)
4. Surgeon - Generates training recipes with adaptive hyperparameters

Your job is to create an intelligent treatment plan that:
- Prioritizes based on severity
- Ensures skill preservation
- Optimizes for success rate
- Coordinates agent collaboration

Respond in JSON format with:
{
  "strategy": "brief strategy description",
  "steps": [
    {
      "agent_name": "diagnostician|pharmacist|neurologist|surgeon",
      "action": "what to do",
      "instructions": "detailed instructions",
      "priority": 1-5,
      "dependencies": ["previous_step_names"]
    }
  ],
  "estimated_time": minutes,
  "confidence": 0.0-1.0,
  "reasoning": "why this plan"
}"""
        
        prompt = f"""{context}

Create an optimal treatment plan for this patient. Consider:
1. What tests should we run first?
2. How severe might this be?
3. What cure strategy would work best?
4. What skills need protection?
5. How should agents collaborate?

Generate a structured treatment plan."""
        
        response = self.llm.generate(prompt, system_prompt=system_prompt, temperature=0.3)
        
        # Parse LLM response
        plan = self._parse_treatment_plan(response, model_id, symptom)
        
        # Display plan
        self._display_plan(plan)
        
        self.current_plan = plan
        return plan
    
    def coordinate_step(self, step: TreatmentStep, previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Coordinate a single treatment step with LLM guidance
        
        Args:
            step: Treatment step to execute
            previous_results: Results from previous steps
            
        Returns:
            Step execution result
        """
        console.print(f"\nðŸ¤– [bold]Coordinator:[/bold] Executing {step.agent_name} - {step.action}")
        
        # Create instruction message
        message = AgentMessage(
            sender="coordinator",
            receiver=step.agent_name,
            message_type=MessageType.INSTRUCTION,
            content=step.instructions,
            metadata={
                "action": step.action,
                "priority": step.priority,
                "previous_results": previous_results
            },
            timestamp=datetime.now().isoformat()
        )
        
        self.conversation_history.append(message)
        
        # Display instruction
        console.print(f"[dim]ðŸ“‹ Instructions: {step.instructions}[/dim]")
        
        return {
            "step": step.action,
            "agent": step.agent_name,
            "instructions": step.instructions,
            "status": "ready"
        }
    
    def analyze_result(self, agent_name: str, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Use Groq LLM to analyze agent result and provide feedback
        
        Args:
            agent_name: Name of agent that produced result
            result: Result to analyze
            
        Returns:
            Analysis with feedback and recommendations
        """
        console.print(f"\nðŸ¤– [bold]Coordinator:[/bold] Analyzing {agent_name} result...")
        
        system_prompt = """You are analyzing results from AI model repair agents.

Evaluate the result for:
1. Quality - Is it good enough?
2. Concerns - Any issues or risks?
3. Revision - Should the agent revise?
4. Feedback - What would improve it?

Respond in JSON format:
{
  "quality_score": 0.0-1.0,
  "concerns": ["list of concerns"],
  "needs_revision": true/false,
  "feedback": "specific feedback",
  "next_action": "recommendation"
}"""
        
        prompt = f"""Analyze this result from {agent_name}:

{json.dumps(result, indent=2)}

Provide quality assessment and feedback."""
        
        response = self.llm.generate(prompt, system_prompt=system_prompt, temperature=0.2)
        
        # Parse analysis
        analysis = self._parse_analysis(response)
        
        # Display analysis
        self._display_analysis(agent_name, analysis)
        
        # Record feedback message
        if analysis.get("feedback"):
            message = AgentMessage(
                sender="coordinator",
                receiver=agent_name,
                message_type=MessageType.FEEDBACK,
                content=analysis["feedback"],
                metadata={"analysis": analysis},
                timestamp=datetime.now().isoformat()
            )
            self.conversation_history.append(message)
        
        return analysis
    
    def synthesize_results(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Use Groq LLM to synthesize all agent results into final recommendation
        
        Args:
            all_results: Results from all agents
            
        Returns:
            Synthesized final result with recommendations
        """
        console.print("\nðŸ¤– [bold]Coordinator:[/bold] Synthesizing final results...")
        
        system_prompt = """You are synthesizing results from multiple AI model repair agents.

Create a comprehensive summary that:
1. Highlights key findings
2. Assesses overall success probability
3. Provides clear next steps
4. Identifies any risks

Respond in JSON format:
{
  "summary": "brief summary",
  "success_probability": 0.0-1.0,
  "key_findings": ["finding1", "finding2"],
  "risks": ["risk1", "risk2"],
  "next_steps": ["step1", "step2"],
  "confidence": 0.0-1.0
}"""
        
        prompt = f"""Synthesize these agent results:

{json.dumps(all_results, indent=2)}

Provide comprehensive summary and recommendations."""
        
        response = self.llm.generate(prompt, system_prompt=system_prompt, temperature=0.3)
        
        # Parse synthesis
        synthesis = self._parse_synthesis(response)
        
        # Display synthesis
        self._display_synthesis(synthesis)
        
        return synthesis
    
    def request_collaboration(self, from_agent: str, to_agent: str, 
                            request: str) -> str:
        """
        Route collaboration request between agents using LLM
        
        Args:
            from_agent: Requesting agent
            to_agent: Target agent
            request: Collaboration request
            
        Returns:
            Routed and potentially refined request
        """
        console.print(f"\nðŸ¤– [bold]Coordinator:[/bold] Routing request: {from_agent} â†’ {to_agent}")
        
        system_prompt = """You are routing collaboration requests between AI agents.

Transform the request to be:
1. Clear and actionable
2. Contextually appropriate
3. Technically precise

Respond with the refined request."""
        
        prompt = f"""Route this collaboration request:

From: {from_agent}
To: {to_agent}
Request: {request}

Refine and route the request."""
        
        response = self.llm.generate(prompt, system_prompt=system_prompt, temperature=0.5)
        
        # Record message
        message = AgentMessage(
            sender=from_agent,
            receiver=to_agent,
            message_type=MessageType.REQUEST,
            content=response,
            metadata={"original_request": request},
            timestamp=datetime.now().isoformat()
        )
        self.conversation_history.append(message)
        
        console.print(f"[dim]ðŸ“¨ Routed: {response[:100]}...[/dim]")
        
        return response
    
    def _parse_treatment_plan(self, response: str, model_id: str, symptom: str) -> TreatmentPlan:
        """Parse LLM response into TreatmentPlan"""
        try:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
            else:
                # Fallback to default plan
                data = self._default_plan()
            
            steps = [
                TreatmentStep(
                    agent_name=step["agent_name"],
                    action=step["action"],
                    instructions=step["instructions"],
                    priority=step.get("priority", 3),
                    dependencies=step.get("dependencies", [])
                )
                for step in data.get("steps", [])
            ]
            
            return TreatmentPlan(
                model_id=model_id,
                symptom=symptom,
                strategy=data.get("strategy", "Standard treatment protocol"),
                steps=steps,
                estimated_time=data.get("estimated_time", 30),
                confidence=data.get("confidence", 0.8),
                reasoning=data.get("reasoning", "Following standard protocol")
            )
            
        except Exception as e:
            console.print(f"[yellow]âš ï¸ Error parsing plan: {e}. Using default.[/yellow]")
            return self._default_treatment_plan(model_id, symptom)
    
    def _parse_analysis(self, response: str) -> Dict[str, Any]:
        """Parse LLM analysis response"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        return {
            "quality_score": 0.8,
            "concerns": [],
            "needs_revision": False,
            "feedback": response[:200],
            "next_action": "proceed"
        }
    
    def _parse_synthesis(self, response: str) -> Dict[str, Any]:
        """Parse LLM synthesis response"""
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except:
            pass
        
        return {
            "summary": response[:200],
            "success_probability": 0.85,
            "key_findings": ["Treatment plan generated"],
            "risks": [],
            "next_steps": ["Execute training"],
            "confidence": 0.85
        }
    
    def _default_treatment_plan(self, model_id: str, symptom: str) -> TreatmentPlan:
        """Generate default treatment plan"""
        steps = [
            TreatmentStep(
                agent_name="diagnostician",
                action="comprehensive_diagnosis",
                instructions=f"Run comprehensive diagnosis on {model_id} for {symptom} issues",
                priority=5,
                dependencies=[]
            ),
            TreatmentStep(
                agent_name="pharmacist",
                action="generate_cure_data",
                instructions=f"Generate cure dataset for {symptom} with 100-200 examples",
                priority=4,
                dependencies=["comprehensive_diagnosis"]
            ),
            TreatmentStep(
                agent_name="neurologist",
                action="check_skills",
                instructions="Verify skill preservation across all domains",
                priority=4,
                dependencies=["generate_cure_data"]
            ),
            TreatmentStep(
                agent_name="surgeon",
                action="generate_recipe",
                instructions="Create training recipe with adaptive hyperparameters",
                priority=3,
                dependencies=["check_skills"]
            )
        ]
        
        return TreatmentPlan(
            model_id=model_id,
            symptom=symptom,
            strategy="Standard comprehensive treatment",
            steps=steps,
            estimated_time=25,
            confidence=0.8,
            reasoning="Following standard protocol for model repair"
        )
    
    def _display_plan(self, plan: TreatmentPlan):
        """Display treatment plan"""
        console.print(f"\n[bold green]ðŸ“‹ Treatment Plan Generated[/bold green]")
        console.print(f"[bold]Strategy:[/bold] {plan.strategy}")
        console.print(f"[bold]Confidence:[/bold] {plan.confidence:.0%}")
        console.print(f"[bold]Estimated Time:[/bold] {plan.estimated_time} minutes")
        console.print(f"\n[bold]Reasoning:[/bold]")
        console.print(f"[dim]{plan.reasoning}[/dim]")
        console.print(f"\n[bold]Steps:[/bold]")
        for i, step in enumerate(plan.steps, 1):
            console.print(f"  {i}. {step.agent_name}: {step.action}")
    
    def _display_analysis(self, agent_name: str, analysis: Dict[str, Any]):
        """Display result analysis"""
        quality = analysis.get("quality_score", 0.8)
        quality_color = "green" if quality > 0.7 else "yellow" if quality > 0.5 else "red"
        
        console.print(f"[{quality_color}]Quality Score: {quality:.0%}[/{quality_color}]")
        
        if analysis.get("concerns"):
            console.print("[yellow]Concerns:[/yellow]")
            for concern in analysis["concerns"]:
                console.print(f"  âš ï¸ {concern}")
        
        if analysis.get("needs_revision"):
            console.print(f"[yellow]ðŸ”„ Requesting revision from {agent_name}[/yellow]")
        else:
            console.print("[green]âœ… Result approved, proceeding[/green]")
    
    def _display_synthesis(self, synthesis: Dict[str, Any]):
        """Display final synthesis"""
        console.print(Panel.fit(
            f"[bold green]ðŸŽ¯ Final Synthesis[/bold green]\n\n"
            f"[bold]Summary:[/bold] {synthesis.get('summary', 'N/A')}\n\n"
            f"[bold]Success Probability:[/bold] {synthesis.get('success_probability', 0.8):.0%}\n\n"
            f"[bold]Confidence:[/bold] {synthesis.get('confidence', 0.8):.0%}",
            border_style="green"
        ))
        
        if synthesis.get("key_findings"):
            console.print("\n[bold]Key Findings:[/bold]")
            for finding in synthesis["key_findings"]:
                console.print(f"  âœ“ {finding}")
        
        if synthesis.get("next_steps"):
            console.print("\n[bold]Next Steps:[/bold]")
            for step in synthesis["next_steps"]:
                console.print(f"  â†’ {step}")
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Get all agent communication history"""
        return [msg.to_dict() for msg in self.conversation_history]
    
    def save_conversation_history(self, output_path: str):
        """Save conversation history to file"""
        with open(output_path, 'w') as f:
            json.dump(self.get_conversation_history(), f, indent=2)
        console.print(f"[green]âœ… Conversation history saved to {output_path}[/green]")


# Export
__all__ = ["CoordinatorAgent", "GroqLLM", "TreatmentPlan", "AgentMessage", "MessageType"]
